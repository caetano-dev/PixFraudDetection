# PIX Fraud Detection System - AI Agent Instructions

This document provides essential guidance for AI agents working on the PIX Fraud Detection System codebase. Understanding these concepts is crucial for providing effective and relevant assistance.

## 1. Architecture & Data Flow

The system is a multi-stage data processing pipeline designed to detect financial fraud. Each Python script represents a distinct stage in the pipeline, and data flows sequentially from one stage to the next, primarily through CSV files and a Neo4j database.

**Core Pipeline Stages:**

1.  **`data_generator.py`**: Creates synthetic, realistic PIX account and transaction data, including complex fraud patterns like smurfing and circular payments.
2.  **`stream_simulator.py`**: Publishes the generated transactions to a Redis channel to simulate a real-time stream.
3.  **`ingestion_engine.py`**: Subscribes to the Redis stream and ingests the transaction data into the Neo4j graph database.
4.  **`feature_engineering.py`**: Queries the Neo4j database to calculate a wide range of behavioral features for each account (e.g., transaction velocity, graph-based metrics). The results are saved to `data/account_features.csv`.
5.  **`community_detection.py` & `local_outlier_factor.py`**: These scripts perform advanced, graph-based fraud detection. They first identify communities (dense clusters of accounts) using the Louvain algorithm and then apply the Local Outlier Factor (LOF) model to find anomalies *within* those communities.
6.  **`anomaly_detection.py`**: Applies standard anomaly detection models (`IsolationForest`, `LocalOutlierFactor`) to the features generated by `feature_engineering.py`. This serves as a baseline or alternative detection method.
7.  **`dashboard.py`**: A Streamlit application for visualizing fraud alerts, exploring communities, and investigating suspicious accounts.

**Data Storage:**
- **Neo4j**: The primary database for storing accounts and transactions as a graph. All graph-related queries use the Cypher language.
- **Redis**: Acts as a real-time message broker for the transaction stream.
- **`./data/` directory**: Contains intermediate CSV files (e.g., `account_features.csv`, `anomaly_scores.csv`) that are used as inputs/outputs between pipeline stages.
- **`./models/` directory**: Stores serialized machine learning models and scalers using `joblib`.

## 2. Key Conventions & Patterns

- **Centralized Configuration**: All database credentials and sensitive parameters are managed through `config.yaml`. The `config.py` module provides a global `config` object to access these settings. When modifying code that connects to external services, always use the `config` object instead of environment variables or hardcoded values.
- **Import-Safe Scripts**: All executable scripts (`data_generator.py`, `ingestion_engine.py`, etc.) are structured with a `main()` function and an `if __name__ == "__main__":` block. This is a critical pattern that allows their functions to be imported for unit testing without triggering the full script execution.
- **Pandas for Data Manipulation**: `pandas` DataFrames are the standard for handling tabular data throughout the project.
- **Command-Line Interfaces**: Scripts use Python's `argparse` library to allow for flexible execution (e.g., selecting an algorithm with `--algorithm` in `anomaly_detection.py`).

## 3. Developer Workflow: Testing

The project uses Python's built-in `unittest` framework and the `unittest.mock` library.

- **Test Location**: All tests are located in the `tests/` directory. Each test file corresponds to a script in the root directory (e.g., `tests/test_anomaly_detection.py` tests `anomaly_detection.py`).
- **Mocking is Essential**: When writing tests, **always** mock external dependencies. This includes:
    - Database connections (`neo4j.GraphDatabase.driver`)
    - Redis connections (`redis.Redis`)
    - Filesystem operations (`pandas.to_csv`, `joblib.dump`)
    - Calls to other pipeline scripts.
- **Testing `main()` Functions**: To test a script's main logic, patch the argument parser (`argparse.ArgumentParser.parse_args`) and the functions that perform the core work. This isolates the main function's orchestration logic. For example, in `test_main_with_isolation_forest` in `tests/test_anomaly_detection.py`, `apply_isolation_forest` is mocked.
- **Running Tests**: Execute tests from the root directory of the project.
  ```bash
  python -m unittest discover tests
  ```
